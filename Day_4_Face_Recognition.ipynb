{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day 4: Face Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVib5A5bCzQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmGMynjF_Ezl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import model as embedding\n",
        "import torch\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysv2Ycm7C7KT",
        "colab_type": "code",
        "outputId": "085f5fcf-f9d5-4ecd-8a19-733a2f9f5242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# face detection model paths\n",
        "protoPath = os.getcwd()+\"/deploy.prototxt.txt\"\n",
        "modelPath = os.getcwd()+\"/res10_300x300_ssd_iter_140000.caffemodel\"\n",
        "# loading detection model\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "# load embedding model\n",
        "embedder = embedding.InceptionResnetV1(pretrained='vggface2').eval()\n",
        "# paths to save pickle files\n",
        "currentDir = os.getcwd()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading parameters (1/2)\n",
            "Downloading parameters (2/2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkm7JGfKDEEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7g1dzL-D5Jw",
        "colab_type": "code",
        "outputId": "8c14de8e-5c30-404d-d00e-2d642d58a87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        }
      },
      "source": [
        "!unzip /content/data1.zip -d /content/dataset/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/data1.zip\n",
            "   creating: /content/dataset/data1/Meet/\n",
            "  inflating: /content/dataset/data1/Meet/WIN_20190517_13_10_18_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Meet/WIN_20190517_13_10_22_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Meet/WIN_20190517_13_10_24_Pro.jpg  \n",
            "   creating: /content/dataset/data1/Tejas/\n",
            "  inflating: /content/dataset/data1/Tejas/WIN_20190517_13_09_34_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Tejas/WIN_20190517_13_09_43_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Tejas/WIN_20190517_13_09_46_Pro.jpg  \n",
            "   creating: /content/dataset/data1/Pranav/\n",
            "  inflating: /content/dataset/data1/Pranav/WIN_20190517_13_09_00_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Pranav/WIN_20190517_13_09_02_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Pranav/WIN_20190517_13_09_04_Pro.jpg  \n",
            "   creating: /content/dataset/data1/Shefali/\n",
            "  inflating: /content/dataset/data1/Shefali/WIN_20190517_14_34_27_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Shefali/WIN_20190517_14_34_30_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Shefali/WIN_20190517_14_34_33_Pro.jpg  \n",
            "   creating: /content/dataset/data1/Devansh/\n",
            "  inflating: /content/dataset/data1/Devansh/WIN_20190517_13_08_48_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Devansh/WIN_20190517_13_08_51_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Devansh/WIN_20190517_13_08_53_Pro.jpg  \n",
            "   creating: /content/dataset/data1/Farzad/\n",
            "  inflating: /content/dataset/data1/Farzad/WIN_20190517_13_09_57_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Farzad/WIN_20190517_13_10_00_Pro.jpg  \n",
            "  inflating: /content/dataset/data1/Farzad/WIN_20190517_13_10_03_Pro.jpg  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AotMQ6TCETpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# images folder\n",
        "dataset = os.path.join(currentDir, \"dataset/data1\")\n",
        "# paths to save pickle files\n",
        "!mkdir output\n",
        "!touch  /content/output/SimpleEmbeddings.pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGzySCtxEX-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting all images paths\n",
        "\n",
        "imagePaths = []\n",
        "\n",
        "for person in os.listdir(dataset):\n",
        "  for img in os.listdir(dataset+\"/\"+person):\n",
        "    imagePaths.append(dataset+\"/\"+person+\"/\"+img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l02WWGLmjEnh",
        "colab_type": "code",
        "outputId": "752437d7-745a-401c-ba3d-2b7af2dfcdef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(imagePaths)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/content/dataset/data1/Farzad/WIN_20190517_13_09_57_Pro.jpg', '/content/dataset/data1/Farzad/WIN_20190517_13_10_03_Pro.jpg', '/content/dataset/data1/Farzad/WIN_20190517_13_10_00_Pro.jpg', '/content/dataset/data1/Pranav/WIN_20190517_13_09_04_Pro.jpg', '/content/dataset/data1/Pranav/WIN_20190517_13_09_00_Pro.jpg', '/content/dataset/data1/Pranav/WIN_20190517_13_09_02_Pro.jpg', '/content/dataset/data1/Meet/WIN_20190517_13_10_18_Pro.jpg', '/content/dataset/data1/Meet/WIN_20190517_13_10_24_Pro.jpg', '/content/dataset/data1/Meet/WIN_20190517_13_10_22_Pro.jpg', '/content/dataset/data1/Shefali/WIN_20190517_14_34_27_Pro.jpg', '/content/dataset/data1/Shefali/WIN_20190517_14_34_30_Pro.jpg', '/content/dataset/data1/Shefali/WIN_20190517_14_34_33_Pro.jpg', '/content/dataset/data1/Devansh/WIN_20190517_13_08_51_Pro.jpg', '/content/dataset/data1/Devansh/WIN_20190517_13_08_53_Pro.jpg', '/content/dataset/data1/Devansh/WIN_20190517_13_08_48_Pro.jpg', '/content/dataset/data1/Tejas/WIN_20190517_13_09_43_Pro.jpg', '/content/dataset/data1/Tejas/WIN_20190517_13_09_46_Pro.jpg', '/content/dataset/data1/Tejas/WIN_20190517_13_09_34_Pro.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiDsNO72FMOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create lists to append ImgPaths/names/imageIDs/boxs/embeddings\n",
        "ImgPaths = []\n",
        "names = []\n",
        "imageIDs = []\n",
        "boxs = []\n",
        "embeddings = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEDcdJUTFObW",
        "colab_type": "code",
        "outputId": "b04a9333-d690-4647-ad20-02d2212772fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# initlize the total number of faces processed\n",
        "total = 0\n",
        "\n",
        "# loop over the image paths\n",
        "for (i, imagePath) in enumerate(imagePaths):\n",
        "  \n",
        "    print(i,imagePath)\n",
        "    \n",
        "    #extract the person name from the image path\n",
        "    \n",
        "    name = imagePath.split(os.path.sep)[-2]\n",
        "    imageID = imagePath.split(os.path.sep)[-1].split('.')[-2]\n",
        "    \n",
        "    image = cv2.imread(imagePath)\n",
        "    (h,w) = image.shape[:2]\n",
        "    \n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "    \n",
        "    detector.setInput(blob)\n",
        "    detections = detector.forward()\n",
        "    \n",
        "    if len(detections) > 0:\n",
        "        \n",
        "        # we're making the assumption that each image has only ONE\n",
        "        # face, so find the bounding box with the largest probalility\n",
        "        \n",
        "        i = np.argmax(detections[0, 0, :, 2])\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "        \n",
        "        # ensure that the detection with the largest probability also\n",
        "        # means our minimum probability test (thus helping filter out\n",
        "        # weak detections)\n",
        "        \n",
        "        if confidence > 0.5:\n",
        "            \n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            \n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            \n",
        "            face = image[startY:endY , startX:endX]\n",
        "            (fH , fW) = face.shape[:2]\n",
        "            \n",
        "            \n",
        "            # ensure the facce width and height are sufficently large\n",
        "            if fW < 20 or fH < 20:\n",
        "                continue\n",
        "                \n",
        "            try:\n",
        "                faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n",
        "            except:\n",
        "                print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
        "                continue\n",
        "            \n",
        "            faceTensor = torch.tensor(faceBlob)\n",
        "            faceEmbed = embedder(faceTensor)\n",
        "            flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n",
        "            \n",
        "            ImgPaths.append(imagePath)\n",
        "            imageIDs.append(imageID)\n",
        "            names.append(name)\n",
        "            boxs.append(box)\n",
        "            embeddings.append(flattenEmbed)\n",
        "            total += 1"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 /content/dataset/data1/Farzad/WIN_20190517_13_09_57_Pro.jpg\n",
            "1 /content/dataset/data1/Farzad/WIN_20190517_13_10_03_Pro.jpg\n",
            "2 /content/dataset/data1/Farzad/WIN_20190517_13_10_00_Pro.jpg\n",
            "3 /content/dataset/data1/Pranav/WIN_20190517_13_09_04_Pro.jpg\n",
            "4 /content/dataset/data1/Pranav/WIN_20190517_13_09_00_Pro.jpg\n",
            "5 /content/dataset/data1/Pranav/WIN_20190517_13_09_02_Pro.jpg\n",
            "6 /content/dataset/data1/Meet/WIN_20190517_13_10_18_Pro.jpg\n",
            "7 /content/dataset/data1/Meet/WIN_20190517_13_10_24_Pro.jpg\n",
            "8 /content/dataset/data1/Meet/WIN_20190517_13_10_22_Pro.jpg\n",
            "9 /content/dataset/data1/Shefali/WIN_20190517_14_34_27_Pro.jpg\n",
            "10 /content/dataset/data1/Shefali/WIN_20190517_14_34_30_Pro.jpg\n",
            "11 /content/dataset/data1/Shefali/WIN_20190517_14_34_33_Pro.jpg\n",
            "12 /content/dataset/data1/Devansh/WIN_20190517_13_08_51_Pro.jpg\n",
            "13 /content/dataset/data1/Devansh/WIN_20190517_13_08_53_Pro.jpg\n",
            "14 /content/dataset/data1/Devansh/WIN_20190517_13_08_48_Pro.jpg\n",
            "15 /content/dataset/data1/Tejas/WIN_20190517_13_09_43_Pro.jpg\n",
            "16 /content/dataset/data1/Tejas/WIN_20190517_13_09_46_Pro.jpg\n",
            "17 /content/dataset/data1/Tejas/WIN_20190517_13_09_34_Pro.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W02CQDJAFQcw",
        "colab_type": "code",
        "outputId": "214cb779-f0f6-403a-c39e-ddc9a50626e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# paths to embedding pickle file\n",
        "embeddingPickle = \"./output/SimpleEmbeddings.pickle\"\n",
        "\n",
        "# dump the facial embeddings + names to disk\n",
        "print(\"[INFO] serializing {} encodings ....\".format(total))\n",
        "data = {\"paths\":ImgPaths, \"names\":names, \"imageIDs\":imageIDs, \"boxs\":boxs, \"embeddings\":embeddings}\n",
        "with open(embeddingPickle , \"wb\") as f:\n",
        "  pickle.dump(data,f)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] serializing 18 encodings ....\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGMxJBzTFSws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# path to recognizer pickle file\n",
        "!touch /content/output/SimpleRecognizer.pickle\n",
        "recognizerPickle = \"./output/SimpleRecognizer.pickle\"\n",
        "\n",
        "# path to labels pickle file\n",
        "!touch /content/output/SimpleLabel.pickle\n",
        "labelPickle = \"./output/SimpleLabel.pickle\"\n",
        "\n",
        "# loading embeddings pickle\n",
        "data = pickle.loads(open(embeddingPickle, \"rb\").read())\n",
        "\n",
        "# encode the labels\n",
        "label = LabelEncoder()\n",
        "labels = label.fit_transform(data[\"names\"])\n",
        "\n",
        "# getting embeddings\n",
        "Embeddings = np.array(data[\"embeddings\"])\n",
        "\n",
        "# train the model used to accept the 512-d embeddings of the face and \n",
        "# then produce the actual face recognition\n",
        "\n",
        "recognizer = KNeighborsClassifier(n_neighbors= 2, metric='euclidean', weights=\"distance\")\n",
        "#recognizer = SVC(probability=True)\n",
        "recognizer.fit(Embeddings, labels)\n",
        "\n",
        "# write the actual face recognition model to disk\n",
        "f = open(recognizerPickle, \"wb\")\n",
        "f.write(pickle.dumps(recognizer))\n",
        "f.close()\n",
        "\n",
        "# write the label encoder to disk\n",
        "f = open(labelPickle,\"wb\")\n",
        "f.write(pickle.dumps(label))\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4lo9J38Fpm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading face detection model\n",
        "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "# load embedding model\n",
        "embedder = embedding.InceptionResnetV1(pretrained=\"vggface2\").eval()\n",
        "\n",
        "# load the actual face recognition model along with the label encoder\n",
        "recognizer = pickle.loads(open(recognizerPickle, \"rb\").read())\n",
        "label = pickle.loads(open(labelPickle, \"rb\").read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXQIzFzHiwZr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d052af6b-bbcb-49d6-cfc9-31b34a6984bb"
      },
      "source": [
        "imagePath = os.getcwd() + \"/test2.jpg\"\n",
        "\n",
        "predictedImg = os.getcwd()\n",
        "\n",
        "image = cv2.imread(imagePath)\n",
        "(h,w) = image.shape[:2]\n",
        "\n",
        "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
        "\n",
        "detector.setInput(blob)\n",
        "detections = detector.forward()\n",
        "\n",
        "# loop over the detections\n",
        "for i in range(0, detections.shape[2]):\n",
        "    \n",
        "    # extract the confidence (i.e., probalility) associated with the prediction\n",
        "    confidence = detections[0, 0, i, 2]\n",
        "    \n",
        "    # fillter out weak detections\n",
        "    if confidence > 0.2:\n",
        "        \n",
        "        # compute the (x ,y) - coordinates of the bounding box for the face\n",
        "        \n",
        "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "        \n",
        "        # extract the face ROI\n",
        "        face = image[startY:endY , startX:endX]\n",
        "        (fH ,fW) = face.shape[:2]\n",
        "        \n",
        "        # ensure the facce width and height are sufficently large\n",
        "        if fW < 20 or fH < 20:\n",
        "            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
        "            continue\n",
        "        \n",
        "\n",
        "        try:\n",
        "            faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255,(160, 160), (0, 0, 0), swapRB=True, crop=False)\n",
        "        except:\n",
        "            print(\"[Error] - Face size in Image not sufficent to get Embeddings : \", imagePath)\n",
        "            continue\n",
        "        \n",
        "        faceTensor = torch.tensor(faceBlob)\n",
        "        faceEmbed = embedder(faceTensor)\n",
        "        flattenEmbed = faceEmbed.squeeze(0).detach().numpy()\n",
        "        \n",
        "        array = np.array(flattenEmbed).reshape(1,-1)\n",
        "        \n",
        "        # perform classification to recognize the face\n",
        "        \n",
        "        preds = recognizer.predict_proba(array)[0]\n",
        "        \n",
        "        j = np.argmax(preds)\n",
        "        \n",
        "        proba = preds[j]\n",
        "        name = label.classes_[j]\n",
        "        \n",
        "        #draw the bunding box of the face along with the associated probability\n",
        "        \n",
        "        text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
        "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
        "        \n",
        "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
        "        \n",
        "        cv2.putText(image, text, (startX, y),cv2.FONT_HERSHEY_SIMPLEX, 0.50, (255, 255, 255), 1)\n",
        "        \n",
        "# save image predicte folder\n",
        "cv2.imwrite(\"{}/test_prediction.png\".format(predictedImg), image)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKyXfNj9oGrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}